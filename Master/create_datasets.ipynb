{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "This code prepares the model's training datasets. The output format is JSON Lines (jsonl), ideal for training models with large datasets, while keeping the data structure lightweight and easy to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe\n",
      "pe\n",
      "pe\n",
      "pe\n",
      "pe\n",
      "pe\n",
      "pe\n",
      "pe\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ba\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "ro\n",
      "es\n",
      "es\n",
      "es\n",
      "es\n",
      "es\n",
      "es\n",
      "es\n",
      "es\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "pi\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "to\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "ma\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "pr\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "ap\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "sc\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "mt\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rs\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rr\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rn\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "rj\n",
      "df\n",
      "df\n",
      "df\n",
      "df\n",
      "df\n",
      "df\n",
      "df\n",
      "df\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "pb\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "mg\n",
      "al\n",
      "al\n",
      "al\n",
      "al\n",
      "al\n",
      "al\n",
      "al\n",
      "al\n",
      "se\n",
      "se\n",
      "se\n",
      "se\n",
      "se\n",
      "se\n",
      "se\n",
      "se\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "ac\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "pa\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "am\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "ce\n",
      "Filtered data has been saved to dataset_global_sp/dataset_global_sp.jsonl\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "state_product_dict = {\n",
    "    state: list(all_data[all_data['state'] == state]['product'].unique())\n",
    "    for state in all_data['state'].unique()\n",
    "}\n",
    "\n",
    "output_file = 'dataset_global/dataset_global.jsonl'\n",
    "\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as file:\n",
    "    \n",
    "    # ''' \n",
    "    # # INFO: ======== Raw Data ========\n",
    "    # ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "\n",
    "    #         # Filter data for the current state and product\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "    #         sequence = data_filtered['m3'].tolist()\n",
    "    #         json_line = {f'{product}_{state}': sequence}\n",
    "\n",
    "    #         file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "    ''' \n",
    "    # INFO: ======== Remove SP ========\n",
    "    ''' \n",
    "    for state, products in state_product_dict.items():\n",
    "        if state == \"sp\":\n",
    "            continue\n",
    "\n",
    "        for product in products:\n",
    "            \n",
    "            # Filter data for the current state and product\n",
    "            data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "            sequence = data_filtered['m3'].tolist()\n",
    "            json_line = {f'{product}_{state}': sequence}\n",
    "\n",
    "            file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "    ''' \n",
    "    # INFO: ======== MinMaxScaler ========\n",
    "    ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "            \n",
    "    #         data = rolling_window(data_filtered['m3'], 12)\n",
    "    #         print(data)\n",
    "\n",
    "    #         sequence = data.values  \n",
    "\n",
    "    #         print(sequence)\n",
    "            \n",
    "    #         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #         sequence_scaled = scaler.fit_transform(sequence.reshape(-1, 1)).flatten()\n",
    "    #         print(sequence_scaled)\n",
    "            \n",
    "    #         json_line = {\"sequence\": sequence_scaled.tolist()} \n",
    "            \n",
    "    #         file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
