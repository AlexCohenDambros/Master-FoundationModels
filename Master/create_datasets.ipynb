{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "This code prepares the model's training datasets. The output format is JSON Lines (jsonl), ideal for training models with large datasets, while keeping the data structure lightweight and easy to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=\";\")\n",
    "\n",
    "state_product_dict = {\n",
    "    state: list(all_data[all_data['state'] == state]['product'].unique())\n",
    "    for state in all_data['state'].unique()\n",
    "}\n",
    "\n",
    "output_file = 'dataset_global/dataset_global.jsonl'\n",
    "\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as file:\n",
    "\n",
    "    ''' \n",
    "    # INFO: ======== Remove SP ========\n",
    "    ''' \n",
    "    for state, products in state_product_dict.items():\n",
    "        if state == \"sp\":\n",
    "            continue\n",
    "\n",
    "        for product in products:\n",
    "            \n",
    "            # Filter data for the current state and product\n",
    "            data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "            sequence = data_filtered['m3'].tolist()\n",
    "            json_line = {f'{product}_{state}': sequence}\n",
    "\n",
    "            file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "    # ''' \n",
    "    # # INFO: ======== Raw Data ========\n",
    "    # ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "\n",
    "    #         # Filter data for the current state and product\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "\n",
    "    #         sequence = data_filtered['m3'].tolist()\n",
    "    #         json_line = {f'{product}_{state}': sequence}\n",
    "\n",
    "    #         file.write(json.dumps(json_line) + '\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    # INFO: ======== MinMaxScaler ========\n",
    "    ''' \n",
    "    # for state, products in state_product_dict.items():\n",
    "    #     for product in products:\n",
    "    #         data_filtered = all_data[(all_data['state'] == state) & (all_data['product'] == product)]\n",
    "            \n",
    "    #         data = rolling_window(data_filtered['m3'], 12)\n",
    "    #         print(data)\n",
    "\n",
    "    #         sequence = data.values  \n",
    "\n",
    "    #         print(sequence)\n",
    "            \n",
    "    #         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #         sequence_scaled = scaler.fit_transform(sequence.reshape(-1, 1)).flatten()\n",
    "    #         print(sequence_scaled)\n",
    "            \n",
    "    #         json_line = {\"sequence\": sequence_scaled.tolist()} \n",
    "            \n",
    "    #         file.write(json.dumps(json_line) + '\\n')\n",
    "    \n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../database/combined_data.csv', sep=';')\n",
    "\n",
    "states = all_data['state'].unique()\n",
    "\n",
    "for excluded_state in states:\n",
    "    data_filtered_all = all_data[all_data['state'] != excluded_state]\n",
    "\n",
    "    state_product_dict = {\n",
    "        state: list(data_filtered_all[data_filtered_all['state'] == state]['product'].unique())\n",
    "        for state in data_filtered_all['state'].unique()\n",
    "    }\n",
    "\n",
    "    output_file = f'all_datasets_global/dataset_{excluded_state}.jsonl'\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        for state, products in state_product_dict.items():\n",
    "            for product in products:\n",
    "                data_filtered = data_filtered_all[(data_filtered_all['state'] == state) & (data_filtered_all['product'] == product)]\n",
    "                sequence = data_filtered['m3'].tolist()\n",
    "                json_line = {f'sequence': sequence}\n",
    "                file.write(json.dumps(json_line) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
